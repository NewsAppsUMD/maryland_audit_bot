Title: Managing for Results - Performance Measures - Public Safety and Safer Neighborhoods
Date: 03/19/2009
Type: Performance
URL: https://www.ola.state.md.us/umbraco/Api/ReportFile/GetReport?fileId=5a945d3bcc9d72404c150aa2
Extracted: 2025-06-18T11:00:30.517183
--------------------------------------------------------------------------------

--- Page 1 ---
Performance Audit Report
Managing for Results
Performance Measures
Public Safety and Safer Neighborhoods
March 2009
OFFICE OF LEGISLATIVE AUDITS
DEPARTMENT OF LEGISLATIVE SERVICES
MARYLAND GENERAL ASSEMBLY

--- Page 2 ---
• This report and any related follow-up correspondence are available to the public through the
Office of Legislative Audits at 301 West Preston Street, Room 1202, Baltimore, Maryland
21201. The Office may be contacted by telephone at 410-946-5900, 301-970-5900, or 1-877-
486-9964.
• Electronic copies of our audit reports can be viewed or downloaded from our website at
http://www.ola.state.md.us.
• Alternate formats may be requested through the Maryland Relay Service at 1-800-735-2258.
• The Department of Legislative Services – Office of the Executive Director, 90 State Circle,
Annapolis, Maryland 21401 can also assist you in obtaining copies of our reports and related
correspondence. The Department may be contacted by telephone at 410- 946-5400 or 301-
970-5400.

--- Page 3 ---
March 19, 2009
Delegate Steven J. DeBoy, Sr., Co-Chair, Joint Audit Committee
Senator Verna L. Jones, Co-Chair, Joint Audit Committee
Members of Joint Audit Committee
Annapolis, Maryland
Ladies and Gentlemen:
We conducted a performance audit to determine the accuracy of selected
Managing for Results (MFR) performance measure data reported in the Maryland
fiscal year 2009 operating budget request. We also determined whether adequate
control systems were in place for collecting, summarizing, and reporting the
performance measure data.
As requested by the chairmen of the legislative budget committees, we are
systematically auditing the results of the 62 MFR measures contained in the
Managing for Results - State Comprehensive Plan, which was produced by the
Department of Budget and Management (DBM). This audit is the first to be
conducted and focuses on the data reported for 13 measures contained within the
Public Safety and Safer Neighborhoods portion of the State Comprehensive Plan.
The agencies responsible for reporting these results are the Department of State
Police (DSP), the Department of Public Safety and Correctional Services
(DPSCS), the Governor’s Office for Children (GOC), the Department of Human
Resources (DHR), and the Department of Juvenile Services (DJS).
As a result of our audit, we have categorized each measure as either Certified,
Certified with Qualification, Inaccurate, or Factors Prevented Certification as
noted in the following chart. These designations are further described in Exhibit
2.
Level of Certification
Factors
Certified with Performance
Certified Inaccurate Prevented
Qualification Measures Audited
Certification
2 4 2 5 13

--- Page 4 ---
The following primary factors contributed to our inability to certify seven
measures: (1) the reported results were not always consistent with the
performance measure descriptions and definitions, (2) documentation was not
always available to support the reported results, and (3) adequate procedures and
controls were not established to provide reasonable assurance that internal agency
and third party data used for the measure results were reliable.
An Executive Summary of our findings can be found on page 3, immediately following
this cover letter, and our audit scope, objectives, and methodology are explained on page
9. The agencies’ responses to this audit are included as an appendix to this report. We
wish to acknowledge the cooperation extended to us by DSP, DPSCS, GOC, DHR, and
DJS during the audit.
Respectfully submitted,
Bruce A. Myers, CPA
Legislative Auditor
2

--- Page 5 ---
Executive Summary
Background Information
In July 1997, the Governor implemented the Managing for Results (MFR)
initiative, which is a strategic planning process used by department leaders and
others to establish direction and priorities for State programs to achieve
meaningful results. MFR requires State agencies to submit missions, goals,
objectives, and performance measures for each program as part of the annual
budget request. This information may then be considered in determining
Statewide spending priorities and the allocation of resources in agency budgets.
Effective July 1, 2004, the MFR process was established in State law, with DBM
as the lead agency for developing a State comprehensive plan for MFR. The
resultant Managing for Results - State Comprehensive Plan categorizes MFR
goals into five functional areas, referred to as pillars, which contain a total of 62
measures. As requested by the chairmen of the legislative budget committees, we
are systematically auditing these measures. This audit is the first to be conducted
on this basis and focuses on the data reported by five agencies in the Maryland
fiscal year 2009 operating budget request for the 13 measures contained within
the Public Safety and Safer Neighborhoods portion of the Plan (See Exhibit 1).
Conclusions
We concluded that, for the 13 measures tested, 2 were Certified, 4 were
considered Certified with Qualification, 2 were deemed Inaccurate, and 5 were
designated as Factors Prevented Certification. We determined that the reported
results were not always consistent with the performance measure descriptions,
and documentation was not always available to support the reported results. We
also determined that, although the various agencies had certain quality control
processes in place, adequate procedures and controls did not exist for certain
measures to provide reasonable assurance that applicable agency internal data and
third party data used for the measure results were reliable. These results are
further described in the Findings section of this report.
Recommendations
The following detailed recommendations are among those we made to the five
agencies to help strengthen the quality control processes and to improve reporting
for the measures we audited.
• Submit reported results that are prepared in a manner that is consistent
with the performance measure descriptions in the MFR budget documents.
• Retain documentation supporting the reported measure results.
• Establish appropriate procedures and internal controls to verify that data
obtained from agency internal records and third parties are reasonably
accurate.
3

--- Page 6 ---
Findings
Certification Results
Agency,
Performance Measure Level of
Program Name, Results
(See Exhibit 1 for Certification Comments / Causes
and Budget Reported
Definitions) (See Exhibit 2)
Reference1
DSP Firearm homicide rate Calendar Certified
Homeland (per 100,000 Year
Security and population) 2006
Investigation
Bureau 7.27
Book 3, Page 813
DSP Traffic fatality rate Calendar Factors The methodology used to calculate the measure result was not
Field (per 100 million vehicle Year Prevented consistent with the measure definition. Although the definition
Operations miles traveled) 2006 Certification provides that fatalities occurring within 90 days of the traffic
Bureau accident be used in the calculation, DSP only used fatalities
Book 3, Page 806 1.14985 occurring within 30 days of the accident. Also, the system used to
collect data for the measure did not capture the date of death (when
death occurred subsequent to the accident date) to enable a
determination of whether all appropriate traffic fatalities were
included in the measure result.
DPSCS Percent of offenders Fiscal Factors Certain data (that is, female offenders released from the Baltimore
Agency-wide returned to Department Year Prevented City Detention Center) were excluded from the calculation of the
Book 2, Page 585 supervision for a new 2006 Certification measure. Although tracked for male offenders, DPSCS did not
offense within one year maintain data of the number of female offenders released from the
of their release from the 21.2% Center who were under DOC supervision. Also, the methodology
Division of Correction used to calculate the measure was not consistent with the measure
(DOC) description. Although the definition provides that offenders
returned to supervision within one year be considered, DPSCS
included offenders that had been returned to supervision up to one
year and one month after release. Finally, the release dates were
not always accurately recorded in the system used to obtain the
measure data. For example, our test of 49 offenders found that the
release dates for 17 individuals did not agree with the related
release documentation.
1 Reference cited is the Maryland fiscal year 2009 operating budget request.
4

--- Page 7 ---
Certification Results
Agency,
Program Performance Measure Level of
Results
Name, and (See Exhibit 1 for Certification Comments / Causes
Reported
Budget Definitions) (See Exhibit 2)
Reference1
DPSCS Total number of inmates Fiscal Certified with Certain quality control processes were not in place to resolve
Division of who escape Year Qualification differences between two internal records used to record serious
Correction 2007 incidents, including escape incidents, involving inmates under
(DOC) DOC custody. Nevertheless, we were able to determine that the
Headquarters 0 reported result was accurate.
Book 2, Pages 617,
618
Total number of inmates Fiscal Certified with Certain quality control processes were not in place to resolve
who walk off Year Qualification differences between two internal records used to record serious
2007 incidents, including walk-off incidents, involving inmates under
DOC custody. Nevertheless, we were able to determine that the
123 reported result was reasonably accurate.
DPSCS Percent of Proactive Fiscal Inaccurate There were a significant number of errors in the underlying data.
Division of Community Supervision Year Specifically, we tested 35 of the 1,237 PCS closed cases for which
Parole and (PCS) cases closed where 2007 the offenders were identified as having satisfactorily completed
Probation – the offender had treatment during fiscal year 2007. For 24 of the cases, there was no
General satisfactorily completed 42% documentation to establish satisfactory completion of a substance
Administration substance abuse treatment abuse treatment program by the offender. Furthermore, for 16 of
Book 2, Page 700 programs the aforementioned 24 cases, the offenders had not been enrolled in
a substance abuse treatment program.
1 Reference cited is the Maryland fiscal year 2009 operating budget request.
5

--- Page 8 ---
Certification Results
Agency,
Program Performance Measure Level of
Results
Name, and (See Exhibit 1 for Certification Comments / Causes
Reported
Budget Definitions) (See Exhibit 2)
Reference1
Governor’s Violent offense arrest rate Calendar Inaccurate The measure was incorrectly stated since the “per 100,000
Office for for youths between ages 15 Year youths” basis used to calculate the measure was not disclosed in
Children – and 17 2006 the budget book. Also, the Office did not verify the reliability of
Children’s data received from third parties, such as the number of violent
Cabinet 1,018 youth arrests per the Maryland State Police.
Interagency
Fund Percent of public school Academic Certified with Certain quality control processes were not in place to verify the
Book 3, Pages children who report using Year Qualification reliability of data received from third parties, such as student
102, 104
alcohol within the last 30 2004 survey results from the State Department of Education.
days – 12th grade Nevertheless, we were able to determine that the reported result
44.1% was reasonably accurate.
Percent of public school Academic Certified with Certain quality control processes were not in place to verify the
children who report using Year Qualification reliability of data received from third parties, such as student
heroin within the last 30 2004 survey results from the State Department of Education.
days – 10th grade Nevertheless, we were able to determine that the reported result
1.1% was reasonably accurate.
1 Reference cited is the Maryland fiscal year 2009 operating budget request.
6

--- Page 9 ---
Certification Results
Agency,
Program Performance Measure Level of
Results
Name, and (See Exhibit 1 for Certification Comments / Causes
Reported
Budget Definitions) (See Exhibit 2)
Reference1
Governor’s Rate of injury-related Calendar Factors The methodology used to calculate the measure result was not
Office for deaths due to accidents to Year Prevented consistent with the performance measure description. Population
Children – children and youth 2005 Certification data of children from ages 1 to 19 was used to calculate the
Children’s between ages 0 and 19 measure instead of population data of children from ages 0 to 19
Cabinet (per 100,000 children) 9.3 as specified in the measure’s description. Also, quality control
Interagency processes were not in place to verify the reliability of data
Fund received from third parties (such as death certificate data from the
Book 3,Page 104 Department of Health and Mental Hygiene).
DHR Percent of children with Fiscal Factors Documentation of the data used to calculate the performance
Child Welfare recurrence of maltreatment Year Prevented measure results was not retained nor could it be replicated. Also,
Services within 6 months of a first 2007 Certification the computer system (Children’s Electronic Social Services
Book 2, Page 450 occurrence Information Exchange – CHESSIE) that was the source for
7.1% underlying data had problems known to DHR with data
completeness and accuracy.
DHR Percent of current support Federal Certified
Local Child paid Fiscal
Support Year
Enforcement 2007
Administration
Book 2, Page 456 63.77%
1 Reference cited is the Maryland fiscal year 2009 operating budget request.
7

--- Page 10 ---
Certification Results
Agency,
Program Performance Measure Level of
Results
Name, and (See Exhibit 1 for Certification Comments / Causes
Reported
Budget Definitions) (See Exhibit 2)
Reference1
DJS Percent of youth re- Fiscal Factors The methodology used to calculate the measure result was not
Office of the adjudicated and re- Year Prevented consistent with the performance measure description or
Secretary convicted within one year 2006 Certification definitions. Specifically, for 14 of the 29 cases tested, DJS used
Book 3, Page 715 after release (all residential various hearing dates instead of the adjudication dates in the
programs) 20% calculation. For example, the difference in the hearing dates used
for 12 of these cases ranged from 41 days before, to 31 days after,
the adjudication dates. Also, supporting documentation of release
dates was not maintained in the case files for 12 of the 29 cases.
Finally, quality control processes were not in place to verify the
reliability of data received from third parties (such as, certain
conviction data obtained from the Department of Public Safety
and Correctional Services).
1 Reference cited is the Maryland fiscal year 2009 operating budget request.
8

--- Page 11 ---
Scope, Objectives, and Methodology
Scope
Under the authority of the State Government Article, Section 2-1221 of the
Annotated Code of Maryland, we conducted an audit of selected performance
measure results reported in the Maryland fiscal year 2009 operating budget
request. The audit was performed in accordance with generally accepted
government auditing standards. As requested by the chairmen of the legislative
budget committees, we are systematically auditing the performance measures
from the Managing for Results - State Comprehensive Plan produced by the
Department of Budget and Management (DBM). This Plan includes 62
performance measures categorized into five functional areas referred to as pillars.
This audit is the first to be conducted on this basis and focuses on the 13
performance measures from Public Safety and Safer Neighborhoods functional
area as reported by the applicable five agencies in the Maryland fiscal year 2009
operating budget request.
Objectives
The objectives of our audit were (1) to determine whether the most recent actual
measurement results for the selected performance measures were accurately
reported in the agencies’ Maryland fiscal year 2009 operating budget requests,
and (2) to determine whether adequate control systems existed over the collection
and reporting of the data related to the measurement results. Our performance
audit did not include an assessment of whether the performance measures
reviewed were consistent with the goals and objectives of the related programs, or
were meaningful indicators of program performance.
Methodology
To accomplish our objectives, we interviewed personnel of the agencies
responsible for collecting and reporting the measure data, reviewed performance
measure calculations for accuracy, and determined whether these calculations
were consistent with the definitions of the performance measures as noted in
Exhibit 1. We used sampling techniques or other methods as deemed appropriate
to test the related source documents. We also analyzed the applicable agency’s
performance measurement data collection and reporting activities to evaluate
whether proper controls existed to ensure data integrity.
We developed a system to categorize the results of our audit of performance
measures. The four categories represent varying levels of certification of the
accuracy of the performance reported by the agencies. The categories of
performance certification are defined in Exhibit 2. If during the course of our
9

--- Page 12 ---
audit of a measure we found circumstances that would require us to conclude that
the measure was either inaccurate or factors prevented certification, we did not
perform additional audit work that may have disclosed other factors.
Our fieldwork, which included site visits to various DSP, DPSCS, GOC, DHR,
and DJS sites, was conducted during the period from February 2008 to October
2008. The responses from the agencies to our findings and recommendations
appear as an appendix in this report. As prescribed in State Government Article,
Section 2-1224 of the Annotated Code of Maryland, we will advise the agencies
regarding the results of our review of their responses.
10

--- Page 13 ---
Exhibit 1
Definitions of the Public Safety and Safer Neighborhoods
Performance Measures Audited
Page 1 of 4
Performance Measure Definition2
Firearm homicide rate Criminal homicides committed in Maryland divided by the Maryland
(per 100,000 population) population data as reported by the United States Census Bureau.
Traffic fatality rate The number of fatalities related to a traffic collision when the injured
(per 100 million vehicle person dies within 90 days after the collision occurred divided by the
miles traveled) number of vehicle miles traveled annually on Maryland roads as
calculated by the State Highway Administration.
Percent of offenders This recidivism measure presents the number of individuals released
returned to Department during a year who are convicted of a new offense within one year of
supervision for a new their release date, expressed as a percentage. The recidivism rate is
offense within one year of calculated from data in the Offender-Based State Correctional
their release from the Information Systems (OBSCIS) maintained by the Division of
Division of Correction (all Correction and Division of Parole and Probation.
releases)
2 The definitions are substantially derived from those provided to DBM in the State agencies’ Managing
for Results budget submissions and from DBM’s Managing for Results Annual Performance Report.
Additional information, such as data sources, was included in certain definitions in this exhibit for
informational purposes.
11

--- Page 14 ---
Exhibit 1
Definitions of the Public Safety and Safer Neighborhoods
Performance Measures Audited
Page 2 of 4
Performance Measure Definition
Total number of inmates “Escape” means an unauthorized inmate departure from
who escape within the secure perimeter of any administrative, maximum,
medium, or minimum security level facility in the Division of
Correction and all inmate departures (regardless of security
classification) while being escorted or transported in
restraints.
Total number of inmates “Walk-off” means the unauthorized inmate departure from a
who walk off pre-release security level facility and includes an inmate,
classified minimum or pre-release security, who departs while
in the community, without restraints, with or without
supervision.
Percent of Proactive The total number of cases closed with satisfactory completion
Community Supervision of a substance abuse treatment program is divided by the total
(PCS) cases closed where number of cases closed where the offender was required to
the offender had complete a substance abuse treatment program to determine
satisfactorily completed the percentage of offenders in that office who have
substance abuse satisfactorily completed treatment.
treatment programs
Auditor’s Note: “Proactive Community Supervision” (PCS)
means a supervision strategy currently being used by the
Division in five specific offices (Hyattsville, Denton, Silver
Spring, Mondawmin, and Guilford Avenue-Baltimore). As
practiced in these offices, this strategy fosters cognitive
behavioral changes in offenders through a combination of
reduced caseloads, enhanced supervision skills, and the use of
effective community partnerships.
Violent offense arrest The rate per 100,000 of arrests of youth ages 15 to 17 for
rate for youths between violent criminal offenses: murder, forcible rape, robbery, and
ages 15 and 17 aggravated assault. The total of such arrests per the Maryland
State Police Uniform Crime Reports are divided by the
estimates of the population of all youths ages 15 to 17 in
Maryland as prepared by the United States Census Bureau in
conjunction with the National Center for Health Statistics.
12

--- Page 15 ---
Exhibit 1
Definitions of the Public Safety and Safer Neighborhoods
Performance Measures Audited
Page 3 of 4
Performance Measure Definition
Percent of public school Percent of public school students (grade 12) who reported
children who report using alcohol within the last 30 days based on a survey
using alcohol within the administered by the Maryland State Department of Education.
last 30 days – 12th grade The survey had a confidence level of 95% with a confidence
interval of +/- 5%.
Percent of public school Percent of public school students (grade 10) who report using
children who report heroin within the last 30 days based on a survey administered
using heroin within the by the Maryland State Department of Education. The survey
last 30 days – 10th grade had a confidence level of 95% with a confidence interval of
+/- 5%.
Rate of injury-related Rate per 100,000 of injury-related deaths to children ages 0-
deaths due to accidents to 19 due to accidents. Accidental deaths reported by the
children and youth Department of Health and Mental Hygiene’s Vital Statistics
between ages 0 and 19 Administration are divided by the youth population estimates
(per 100,000 children) prepared by the United States Census Bureau in conjunction
with the National Center for Health Statistics.
Percent of children with Maltreatment means physical abuse, neglect, sexual abuse,
recurrence of mental injury abuse or mental injury neglect investigation that
maltreatment within 6 has a finding of either indicated or unsubstantiated and a child
months of a first with a role of victim entered into MD CHESSIE*. The
occurrence denominator is the number of children with a victim code
(victim codes can only be placed on investigations which
close as indicated or unsubstantiated) whose investigation
starts in the first 6 months of the report period. The numerator
is the number of the same children with a victim code whose
investigation started within 6 months of the previous
investigation. The computation is the numerator divided by
the denominator times 100%.
*MD CHESSIE (Maryland Children’s Electronic Social
Services Information Exchange) is an automated system
designed to capture and track child welfare data.
13

--- Page 16 ---
Exhibit 1
Definitions of the Public Safety and Safer Neighborhoods
Performance Measures Audited
Page 4 of 4
Performance Measure Definition
Percent of current This measure was based on activity in DHR’s Child Support
support paid Enforcement System that occurred during a single federal
fiscal year (October 1st through September 30th) that was
reported to the federal government. The reported "Total
Amount of Support Distributed as Current Support During the
Fiscal Year" is divided by the reported "Total Amount of
Current Support Due for the Fiscal Year" to determine the
percent paid.
Percent of youth re- The measure is calculated based on the percentage of youths
adjudicated and re- released from residential programs during a fiscal year that
convicted within one year were either re-adjudicated as a juvenile delinquent, or
after release convicted as an adult within one year after release. DJS counts
(all residential programs) recidivists only once, even if a youth has offended in both
systems. The data for the measure is obtained from DJS’
ASSIST (Automate Statewide Support and Information
System) database.
14

--- Page 17 ---
Exhibit 2
Categories of Performance Certification
Category Definition
Certified Reported performance was reasonably accurate.
Certified with Reported performance was reasonably accurate even though
Qualification minor deficiencies were noted with the supporting
documentation, controls were not sufficient, or the
methodology used to calculate reported performance was not
consistent with the measure definition.
Inaccurate Reported performance differed significantly from actual
performance; the calculation process was wrong, such as
excluding data relevant to the calculation; or, as reported, the
measure was misleading, such as failing to disclose the
measure as a rate when applicable.
Factors Prevented Reported performance could not be verified, as documentation
Certification was unavailable, controls were not adequate to ensure the
accuracy of the results, or results were not presented in a
manner consistent with the performance measure description.
15

--- Page 19 ---
Audit Progress Report -- Department of State Police
Performance Audit Report
Managing for Results Performance Measures
Public Safety and Safer Neighborhoods
February 2009
DSP Field Operations Bureau, Book 3, Page 806
Performance Measure: Traffic fatality rate (per 100 million vehicle miles traveled) 2006
Level of Certification: Factors Prevented Certification
Comments/Causes
The methodology used to calculate the measure result was not consistent with the measure definition.
Although the definition provides that fatalities occurring within 90 days of the traffic accident be used
in the calculation, DSP only used fatalities occurring within 30 days of the accident. Also, the system
used to collect data for the measure did not capture the date of death (when death occurred subsequent
to the accident date) to enable a determination of whether all appropriate traffic fatalities were included
in the measure result.
Department Response
The Department agrees with the certification results. The Department has changed its MFR definition
to reflect the 30 day measurement. The MFR for FY2010 reflects this change. In addition, the
Department currently captures the date of death (when death occurred subsequent to the accident date)
to ensure the accuracy of the measure result. The Department currently uses data from FAARS as well
as its Central Records Division to ensure it can accurately determine the traffic fatality rate. The
FAARS Unit receives information on traffic fatalities from multiple sources. The Department uses
data from both systems in order to ensure that all fatal crashes are reported and in determining specific
time frames for occurrence.

--- Page 21 ---
-2-
month of return. An inmate released in June and returned the following
June is counted as returning ‘within a year of release,’ even if (for
example) the release was June 1 and the return was June 30.”
• The DPDS, DOC, and the Central Home Detention Unit completed a
thorough investigation of the cases identified with inaccurate release
dates in the Department’s computer system (OBSCIS). Inaccurate
dates have been corrected where they were determined to exist.
Because certain releases can be delayed due to transfer of custody
situations over which the DOC had no control, the Department has
documented in the Fiscal Year 2010 Budget Book the following
clarification: “A ‘release’ is counted from the date recorded in the
Department’s Offender-Based State Correctional Information
System (OBSCIS I), which is when an inmate is physically released
from custody. In cases where an inmate can be released only to
another jurisdiction’s detainer (for a court appearance, to serve another
sentence, etc.), this date may be later than the date documented by the
commitment office if the detaining jurisdiction fails to take the inmate
into its custody on the scheduled release date.”
Performance Measure – Total number of inmates who escape
Level of Certification – Certified with Qualification
In July 2008, the Division began ensuring that the reconciliation procedure between
the source documents (OBSCIS and Serious Incident Reports) is documented and
completed timely and that the results support the measure as reported.
Performance Measure – Total number of inmates who walk off
Level of Certification – Certified with Qualification
In July 2008, the Division began ensuring that the reconciliation procedure between
the source documents (OBSCIS and Serious Incident Reports) is documented and
completed timely and that the results support the measure as reported.
Performance Measure – Percent of Proactive Community Supervision (PCS)
cases closed where the offender had satisfactorily completed substance abuse
treatment programs
Level of Certification – Inaccurate
The Division will ensure that its data collection is accurate and report results that are
consistent with the performance measure description and definitions. In February
2009, the Division’s Quality Assurance Unit (QAU) completed a review of a
significant number of closed cases for which the Division’s computer data indicated
satisfactory completion of the special condition for substance abuse treatment. The
QAU reviewed cases from every Division office to determine if the computer data

--- Page 24 ---
Response to Performance Audit Report
Managing for Results Performance Measures
Public Safety and Safer Neighborhoods
By the Governor’s Officer for Children,
On behalf of the Children’s Cabinet
3/16/09
Performance Measure:
Violent offense arrest rate for youths between ages 15 and 17
Results Reported:
Calendar Year 2006 Actual Reported Results – 1,108
Level of Certification:
Inaccurate
Comments / Causes:
The measure was incorrectly stated since the “per 100,000 youths” basis used to calculate
the measure was not disclosed in the budget book. Also, the Office did not verify the
reliability of data received from third parties, such as the number of violent youth arrests
per the Maryland State Police.
Response
The Governor’s Office for Children and the Children’s Cabinet concur with Comments and/or
Causes. The FY11 MFR will be changed to include the designation “per 100,000 youth” to
ensure proper interpretation of the data.
Additionally, to enable GOC to verify the reliability of third-party data used in measurements
and calculations for the MFR, GOC will collect and maintain records of each third-party’s data
definitions and control procedures, where possible (recognizing that such documentation may
not be available from some federal agencies).

--- Page 25 ---
Response to Performance Audit Report
Managing for Results Performance Measures
Public Safety and Safer Neighborhoods
By the Governor’s Officer for Children,
On behalf of the Children’s Cabinet
3/16/09
Performance Measure:
Percent of public school children who report using alcohol within the last 30 days – 12th
grade
Results Reported:
Academic Year 2004 Actual Reported Results – 44.1%
Level of Certification
Certified with Qualification
Comments / Causes:
Certain quality control processes were not in place to verify the reliability of data received
from third parties, such as student survey results from the State Department of Education.
Nevertheless, we were able to determine that the reported result was reasonably accurate.
Response
The Governor’s Office for Children and the Children’s Cabinet concur with Comments and/or
Causes. To enable GOC to verify the reliability of third-party data used in measurements and
calculations for the MFR, GOC will collect and maintain records of each third-party’s data
definitions and control procedures, where possible (recognizing that such documentation may
not be available from some federal agencies).

--- Page 26 ---
Response to Performance Audit Report
Managing for Results Performance Measures
Public Safety and Safer Neighborhoods
By the Governor’s Officer for Children,
On behalf of the Children’s Cabinet
3/16/09
Performance Measure:
Percent of 10th grade public school children who report using heroin within the last 30 days
Results Reported:
Academic Year 2004 Actual Reported Results – 1.1%
Level of Certification
Certified with Qualification
Comments / Causes:
Certain quality control processes were not in place to verify the reliability of data received
from third parties, such as student survey results from the State Department of Education.
Nevertheless, we were able to determine that the reported result was reasonably accurate.
Response
The Governor’s Office for Children and the Children’s Cabinet concur with Comments and/or
Causes. To enable GOC to verify the reliability of third-party data used in measurements and
calculations for the MFR, GOC will collect and maintain records of each third-party’s data
definitions and control procedures, where possible (recognizing that such documentation may
not be available from some federal agencies).

--- Page 27 ---
Response to Performance Audit Report
Managing for Results Performance Measures
Public Safety and Safer Neighborhoods
By the Governor’s Officer for Children,
On behalf of the Children’s Cabinet
3/16/09
Performance Measure:
Rate of injury-related deaths due to accidents to children and youth between ages 0 and 19
(per 100,000 children)
Results Reported:
Calendar Year 2005 – 9.3
Level of Certification:
Factors Prevented Certification
Comments / Causes:
The methodology used to calculate the measure result was not consistent with the
performance measure description. Population data of children from ages 1 to 19 was used
to calculate the measure instead of population data of children from ages 0 to 19 as
specified in the measure’s description. Also, quality control processes were not in place to
verify the reliability of data received from third parties (such as death certificate data from
the Department of Health and Mental Hygiene).
Response
The Governor’s Office for Children and the Children’s Cabinet concur with the comments
and/or Causes. The FY10 MFR calculations were checked by our Director of Research and
Evaluation and the Calendar Year 2005 data represent data on ages 0 to 19 for deaths of
children due to accident. Subsequent years’ data will also match the related measure
description.
Additionally, to enable GOC to verify the reliability of third-party data used in measurements
and calculations for the MFR, GOC will collect and maintain records of each third-party’s data
definitions and control procedures, where possible (recognizing that such documentation may
not be available from some federal agencies).

--- Page 29 ---
DEPARTMENT OF HUMAN RESOURCES
SOCIAL SERVICE ADMINISTRATION (SSA)
RESPONSE TO
DRAFT MANAGING FOR RESULT AUDIT REPORT
OLA’s COMMENTS / CAUSES
Documentation of the data used to calculate the performance measure results was not
retained nor could it be replicated. Also, the computer system (Children’s Electronic
Social Services Information Exchange - CHESSIE) that was the sources for underlying
data had problems known to DHR with data completeness and accuracy.
DHR/SSA RESPONSE:
The Department is continuing to enhance and improve the reliability and accuracy of the
MD CHESSIE. These enhancements will align MD CHESSIE more closely to
Maryland’s child welfare business practice, and will significantly improve the
Department’s federal AFCARS and NCANDS data quality and reporting, and incorporate
Child and Family Services Reviews’ Onsite Review Instrument.
The Department has created new MD CHESSIE reports designed to improve the
accuracy and reliability of reporting in MD CHESSIE. Local jurisdictions have been
producing hand counts to meet the monthly State Stat requirements. These hand counts
are then compared to the new MD CHESSIE State Stat reports to help determine whether
the LDSS hand counts match the State Stat reports generated from MD CHESSIE. Once
100% accuracy is obtained, MD CHESSIE will begin to generate the new Monthly
Accountability Reports, the replacement of legacy system Monthly Management Reports.

--- Page 31 ---
DEPARTMENT OF JUVENILE SERVICES
RESPONSE TO OLA AUDIT OF DJS MFR MEASURE
March 13, 2009
On April 11, 2008, the Office of Legislative Audits (OLA) began an audit of the Department of Juvenile Services
(DJS) recidivism in compliance with the state Managing For Results (MFR) requirements. The OLA auditors
indicated they would be evaluating internal controls, input-output mechanisms, third party data, the calculation of
recidivism, and the validity of the data. Conclusions drawn from the audit were based upon audit findings on 29 out
60 cases. Four areas of concern that may have prevented certification of the DJS MFR recidivism measure were
presented.
1.) The first concern revolves around the date the Department has used to determine whether a youth has returned to
the system. DJS has been using the following definition for re-adjudication: Re-adjudication refers to any juvenile,
who is re-referred, has a judiciary hearing and is adjudicated delinquent. During DJS staff meetings with the OLA
auditor, the process and methodology used to determine recidivism was explained. A step-by-step guide and the
definition used by DJS to qualify the data from ASSIST to determine recidivism was offered. The methodology used
in calculating the rate of recidivism the Department used from ASSIST data was also presented for review.
DJS Staff explained that the disposition date is used for calculating DJS recidivism. Data in the disposition field is
more often filled in by case workers than is the field for the adjudication date. The adjudication date field may
reflect hearings that did not result in a delinquency result, either through postponement or through a result other than
delinquency. DJS staff believe that using the disposition field gives more reliable data, and is only entered for youth
with a final delinquency status. Additionally, due to the structure of the ASSIST database, the linkage between the
individual case and the disposition date is far stronger than that of the adjudication date. As this measure uses a large
data set with thousands of records, an automated process must be used. In the opinion of DJS staff, The ASSIST
disposition date is the most reliable field to use.
It would seem reasonable to audit the accuracy of the ASSIST data stored in the disposition date field to ensure that it
comports with the actual date of disposition as recorded in the case files, as this is the field used in the MFR
recidivism measure. The audit that was performed on the sample of the 29 youths was done comparing the
adjudication date for 21 of the sampled youth to the case file documents not the disposition date. The remaining 8
youth had adult charges following their release from DJS placement and are considered adult recidivism, not juvenile
recidivism. In many cases, especially in certain jurisdictions, the adjudicatory hearings and disposition decision
actually occur on the same date. In all such cases the difference between using the adjudication date and disposition
date appears to be insignificant. From a review of the data, it appears that the result of the overall performance
measure (percent of youth re-adjudicated within one year) would not change, even if the adjudication date as verified
in case files was used.
OLA auditors pointed out, however, that the definition used for DJS recidivism did not comport with the actual
methods used to determine recidivism. To that end, DJS has updated the Recidivism measure for youth who have
been re-adjudicated. The updated definition is as follows.
Re-adjudication refers to any juvenile, who is re-referred, has a judiciary hearing and is adjudicated
delinquent. It should be noted that the term ‘adjudicated’ mentioned in the definition does not imply that the
Department used the adjudication date in ASSIST to count the adjudicated delinquent youths. The
Department used the court disposition date and the court finding of ‘delinquent’ to determine that the youth
has been adjudicated delinquent. All recidivism related data variables are retrieved from the Department’s
computerized system known as ASSIST. The procedure used to calculate the re-adjudication court of youth
is based on several variables and conditions. These are:
Page 1

--- Page 32 ---
• the alleged offense date has to be greater than or equal to the release date from the committed program.
• the complaint date has to be greater than or equal to the offense date.
• the court disposition date has to be greater than or equal to the complaint date and
• the court finding should be “delinquent.”
2.) The second area of concern was the lack of supporting documentation for the “released from a committed
placement.” The auditors required documentation to support the release dates that prove recidivism. We believe that
they excluded narrative from ASSIST citing it as unacceptable evidence/supporting documentation of release. After
some discussion with the OLA auditor, DJS decided to obtain the Certificate of Placement (COP), the document
which reflects a youth’s location, services, and authorized payment for services, to prove release from a committed
placement. A COP is only completed for youth released from privately-operated residential facilities and not for
youth released from state-operated facilities. Following the conversation between DJS staff and the OLA auditor,
DJS decided to obtain court orders to also confirm release dates. Unfortunately DJS was unable to locate the COP
for all youth released from privately operated facilities either at DJS Headquarter or in the field for the cases under
review. DJS is undertaking an intensive reform of its community case management system currently in place. To
better understand the level of case management services, the Department is conducting state-wide case reviews. The
Department is taking aggressive steps to implement corrective action plans that include training, increased resources,
and best practices in case management. To address the failure to regularly complete the COP, the Department will
develop a corrective action plan to ensure the accurate completion of COP according to DJS policy. The corrective
action plan will also include efforts to improve the regular completion of the COP for youth upon their admission and
release from residential placement and will be completed within the next six months.
3.) The third issue of concern was the reliability and validity of the date DJS had received from the Department of
Public Safety and Correctional Services (DPSCS), also referred to as the adult data or third party data. OLA
determined that DJS does not audit or review the date to ensure data reliability or validity. In response to the final
result of the OLA analysis of the DJS recidivism, the DJS Office of Internal Audit (OIA) determined that it must
document and validate third (3rd) party data received from outside sources used for critical decision making.
Accordingly, OIA will establish a regular audit procedure to conduct internal audits semi-annually of the reported
data from DPSCS for the DJS recidivism measures. The OIA also will work with the Maryland State Administrative
Office of the Courts in Annapolis, Judicial Information Systems, to obtain sufficient information relevant to this
analysis, for verification and validation procedures of data associated with this review, to ensure Internal Audit
compliance with Section 3-1002 (d) of the Maryland State Finance and Procurement Article.
4.) The fourth and last issue is the duplication of four cases that were in the data set given to the OLA auditor for
review. These four cases were identified by the Department before the OLA auditor commenced the work. The
auditors were then informed how the duplicate numbers occurred. On 4/14/2008 DJS identified the duplicated four
youth IDs to the OLA auditor, and provided the auditor with the corrected 133 convictions file and the corrected FY
2006 DJS/CJIS adjudications/convictions recidivism rate. DJS staff also reviewed the SPSS analysis process with
the OLA Auditor and pointed out how the four duplicated youths were picked up by mistake in the original report.
The error occurred by not checking a box in the SPSS program after the matching process was done. This error
picked up the four youth who recidivated in both systems within one year after release. Duplicated data is now
deleted from the performance measure results prior to the reporting of recidivism data.
Page 2

--- Page 33 ---
A T
UDIT EAM
Timothy R. Brooks, CPA, CFE
Audit Manager
Nelson W. Hopkins, CPA
Senior Auditor
David S. Propper
Robert A. Wells
Staff Auditors